{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec52db8a",
   "metadata": {},
   "source": [
    "LinkedIn Data Analyst Jobs | ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b9406",
   "metadata": {},
   "source": [
    "Extract info from LinkedIn job postings using Selenium and Beautiful Soup\n",
    "Search Criteria:\n",
    "    -Data Analyst\n",
    "    -Chicago, IL\n",
    "    -Posted in past 24 hours\n",
    "    -Within 25 miles\n",
    "    -Entry level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f14204",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5618389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import config\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05253ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start timer\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a5c39",
   "metadata": {},
   "source": [
    "Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a5de8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Selenium to scroll to bottom of search results page and begin extracting data\n",
    "#url to extract\n",
    "url  = 'https://www.linkedin.com/jobs/search?keywords=Data%20Analyst&location=Chicago%2C%20Illinois%2C%20United%20States&locationId=&geoId=103112676&f_TPR=r86400&distance=25&f_E=2&position=1&pageNum=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a03213fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_jobs(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    #scroll to bottom\n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    class_name = \"job-search-card\"\n",
    "    jobs = driver.find_elements(By.CLASS_NAME,class_name)\n",
    "    for job in jobs:\n",
    "        job_link = job.find_element(By.CLASS_NAME,\"base-card__full-link\").get_attribute('href')\n",
    "        title = job.find_element(By.CLASS_NAME,\"base-search-card__title\").text\n",
    "        company = job.find_element(By.CLASS_NAME,\"hidden-nested-link\").text\n",
    "        company_link = job.find_element(By.CLASS_NAME,\"hidden-nested-link\").get_attribute('href')\n",
    "        location = job.find_element(By.CLASS_NAME,\"job-search-card__location\").text\n",
    "        try:\n",
    "            benefit = job.find_element(By.CLASS_NAME,\"result-benefits__text\").text\n",
    "        except:\n",
    "            benefit = ''\n",
    "        data ={\n",
    "            'date' : date.today(),\n",
    "            'job_link': job_link,\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'company_link': company_link,\n",
    "            'location': location,\n",
    "            'benefit': benefit\n",
    "        }\n",
    "        job_list.append(data)\n",
    "        \n",
    "    driver.close() #close the browser\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46238947",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = []\n",
    "extract_all_jobs(url)\n",
    "df = pd.DataFrame(job_list) #create dataframe with job list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79e5214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Beautiful Soup to scrape each job link of more info\n",
    "def extract_job_details(job_links):\n",
    "    for link in job_links:\n",
    "        #headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}\n",
    "        url  = link\n",
    "        #r = requests.get(url,headers)\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        driver.get(url)\n",
    "        #wait for page load\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'description__job-criteria-list'))\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "        #driver.implicitly_wait(10)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,'html.parser') #read with beautiful soup\n",
    "        #use try loops for each column. when page doesn't load, this will prevent errors and keep row count the same\n",
    "        try:\n",
    "            description = soup.find('div',class_ ='show-more-less-html__markup').text.strip()\n",
    "        except:\n",
    "            description = ''\n",
    "        try:\n",
    "            applicant_caption = soup.find('figcaption',class_ ='num-applicants__caption').text.strip()\n",
    "        except:\n",
    "            applicant_caption = ''\n",
    "        try:\n",
    "            posted_time_ago = soup.find('span',class_ ='posted-time-ago__text posted-time-ago__text--new topcard__flavor--metadata').text.strip()\n",
    "        except:\n",
    "            posted_time_ago = ''\n",
    "        criteria = soup.find_all('li',class_='description__job-criteria-item')\n",
    "        try:\n",
    "            seniority_level = criteria[0].find('span',class_ = 'description__job-criteria-text description__job-criteria-text--criteria').text.strip()\n",
    "        except:\n",
    "            seniority_level = ''\n",
    "        try:\n",
    "            employment_type = criteria[1].find('span',class_ = 'description__job-criteria-text description__job-criteria-text--criteria').text.strip()\n",
    "        except:\n",
    "            employment_type = ''\n",
    "        try:\n",
    "            job_function = criteria[2].find('span',class_ = 'description__job-criteria-text description__job-criteria-text--criteria').text.strip()\n",
    "        except:\n",
    "            job_function = ''\n",
    "        try:\n",
    "            industries = criteria[3].find('span',class_ = 'description__job-criteria-text description__job-criteria-text--criteria').text.strip()\n",
    "        except:\n",
    "            industries = ''\n",
    "            \n",
    "            \n",
    "        #extract additional data from large block of text at bottom of page\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(soup.find('script',{'type':'application/ld+json'}).text)\n",
    "        except:\n",
    "            data = ''\n",
    "        #either block of text does not exist for all links, or some links do not work\n",
    "        #will use try loops for all columns so number of rows match extract_all_jobs\n",
    "        try:\n",
    "            date_posted = data['datePosted'] #begin extracting data using json format\n",
    "        except:\n",
    "            date_posted = ''\n",
    "        try:\n",
    "            description = data['description']\n",
    "        except:\n",
    "            description = ''\n",
    "        try:\n",
    "            employment_type = data['employmentType']\n",
    "        except:\n",
    "            employment_type = ''\n",
    "        try:\n",
    "            industry = data['industry']\n",
    "        except:\n",
    "            industry = ''\n",
    "        try:\n",
    "            latitude = data['jobLocation']['latitude']\n",
    "        except:\n",
    "            latitude = ''\n",
    "        try:\n",
    "            longitude = data['jobLocation']['longitude']\n",
    "        except:\n",
    "            longitude = ''\n",
    "        try:\n",
    "            education_type = data['educationRequirements']['@type']\n",
    "        except:\n",
    "            education_type = ''\n",
    "        try:\n",
    "            education_category = data['educationRequirements']['credentialCategory']\n",
    "        except:\n",
    "            education_category = ''\n",
    "        try:\n",
    "            experience_type = data['experienceRequirements']['@type']\n",
    "        except:\n",
    "            experience_type = ''\n",
    "        try:\n",
    "            experience_time = data['experienceRequirements']['monthsOfExperience']\n",
    "        except:\n",
    "            experience_time = ''\n",
    "        try:\n",
    "            job_location_type = data['jobLocationType']\n",
    "        except:\n",
    "            job_location_type = ''\n",
    "        try:\n",
    "            location_requirement_type = data['applicantLocationRequirements']['@type']\n",
    "        except:\n",
    "            location_requirement_type = ''\n",
    "        try:\n",
    "            location_requirement_name = data['applicantLocationRequirements']['name']\n",
    "        except:\n",
    "            location_requirement_name = ''\n",
    "\n",
    "            \n",
    "        data ={\n",
    "            'description':description,\n",
    "            'applicant_caption':applicant_caption,\n",
    "            'posted_time_ago':posted_time_ago,\n",
    "            'seniority_level':seniority_level,\n",
    "            'employment_type':employment_type,\n",
    "            'job_function':job_function,\n",
    "            'industry':industries,\n",
    "            'date_posted' : date_posted,\n",
    "            'description_json' : description,\n",
    "            'employment_type_json' : employment_type,\n",
    "            'industry_json' : industry,\n",
    "            'latitude' : latitude,\n",
    "            'longitude' : longitude,\n",
    "            'education_type' : education_type,\n",
    "            'education_category' : education_category,\n",
    "            'experience_type' : experience_type,\n",
    "            'experience_time' : experience_time,\n",
    "            'job_location_type' : job_location_type,\n",
    "            'location_requirement_type' : location_requirement_type,\n",
    "            'location_requirement_name' : location_requirement_name,\n",
    "            \n",
    "        }\n",
    "        job_details.append(data)\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea6a1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = []\n",
    "extract_job_details(df['job_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a3432",
   "metadata": {},
   "source": [
    "Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0034a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add extracted columns to job list. since row count is the same, no common key needed\n",
    "df3 = pd.concat([df,\n",
    "           pd.DataFrame(job_details)],\n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48345a64",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96acffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to folder with today's date as filename. these csv's can be saved as backup\n",
    "today = date.today()\n",
    "df3.to_csv('{}{}.csv'.format(config.save_path,today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5da9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load google cloud credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '{}'.format(config.service_path)\n",
    "    ,scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append data to bigquery\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "# Make the API request\n",
    "job_config.create_disposition = \"CREATE_IF_NEEDED\"\n",
    "table_id = \"linkedin_jobs.data_analyst_chicago\"\n",
    "job = client.load_table_from_dataframe(dataframe=df3,\n",
    "                                               destination=table_id, \n",
    "                                               job_config=job_config)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab890b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end timer\n",
    "end_time = time.perf_counter()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
